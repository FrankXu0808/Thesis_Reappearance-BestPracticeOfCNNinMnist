{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import cv2\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import mnist # 导入 pytorch 内置的 mnist 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Elastic_transform(image, alpha, sigma):\n",
    "    shape = image.shape\n",
    "    shape_size = shape[:2]\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "    dz = np.zeros_like(dx)\n",
    "\n",
    "    x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n",
    "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1)), np.reshape(z, (-1, 1))\n",
    "\n",
    "    return map_coordinates(image, indices, order=1, mode='reflect').reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_transform(image, alpha, sigma, random_state=None):\n",
    "    \"\"\"Elastic deformation of images as described in [Simard2003]_.\n",
    "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
    "       Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
    "       Proc. of the International Conference on Document Analysis and\n",
    "       Recognition, 2003.\n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "\n",
    "    shape = image.shape\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dz = np.zeros_like(dx)\n",
    "\n",
    "    x, y, z = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), np.arange(shape[2]))\n",
    "    print(x.shape)\n",
    "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1)), np.reshape(z, (-1, 1))\n",
    " \n",
    "    distored_image = map_coordinates(image, indices,order=1, mode='reflect')\n",
    "    return distored_image.reshape(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic transform\n",
    "def elastic_transformations(alpha, sigma, rng=np.random.RandomState(42), \n",
    "                            interpolation_order=1):\n",
    "    \"\"\"Returns a function to elastically transform multiple images.\"\"\"\n",
    "    # Good values for:\n",
    "    #   alpha: 2000\n",
    "    #   sigma: between 40 and 60\n",
    "    def _elastic_transform_2D(images):\n",
    "        \"\"\"`images` is a numpy array of shape (K, M, N) of K images of size M*N.\"\"\"\n",
    "        # Take measurements\n",
    "        image_shape = images[0].shape\n",
    "        # Make random fields\n",
    "        dx = rng.uniform(-1, 1, image_shape) * alpha\n",
    "        dy = rng.uniform(-1, 1, image_shape) * alpha\n",
    "        # Smooth dx and dy\n",
    "        sdx = gaussian_filter(dx, sigma=sigma, mode='reflect')\n",
    "        sdy = gaussian_filter(dy, sigma=sigma, mode='reflect')\n",
    "        # Make meshgrid\n",
    "        x, y = np.meshgrid(np.arange(image_shape[1]), np.arange(image_shape[0]))\n",
    "        # Distort meshgrid indices\n",
    "        distorted_indices = (y + sdy).reshape(-1, 1), \\\n",
    "                            (x + sdx).reshape(-1, 1)\n",
    "\n",
    "        # Map cooordinates from image to distorted index set\n",
    "        transformed_images = [map_coordinates(image, distorted_indices, mode='reflect',\n",
    "                                              order=interpolation_order).reshape(image_shape)\n",
    "                              for image in images]\n",
    "        return transformed_images\n",
    "    return _elastic_transform_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用内置函数下载 mnist 数据集\n",
    "train_set = mnist.MNIST('./data', train=True, download=True)\n",
    "test_set = mnist.MNIST('./data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/0lEQVR4nGNgGHhgPP/vfCMccgbv/vz58xa7nNnjv3/ev/xjyYYpxWXz4M/fP6dC/vytgggwIUnOPCDDwMBgxHOQQRdD0tibkfFQKeOL85OYGLG5ZTOPd6UoA8Pfz2gOVlv69+WFEAj775+lKHLsm/58cBeWgUkeRpG0/PPHHs5Blzz2dx+C8//vEWTX+hj834SQ/Pf/ArLG0D/PJOHWt//dxYMqeR8u1/znoTsDquREKMtg6Z+1DKgg7O9DCKPo3d9FaHIMoX9+TjKQDd308O/95RaYkn/+PL3+58+fI03oUgwMMsf//Pn758/LiZhSDAwMkg1//v7pVcUqR1cAAKxwbkTVIzd2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1A8C9B70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_data, a_label = train_set[1]\n",
    "a_data\n",
    "#plt.imshow(a_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "def data_tf(x):\n",
    "    x = np.array(x, dtype='float32') / 255\n",
    "    x = (x - 0.5) / 0.5 # 标准化，这个技巧之后会讲到\n",
    "    x = x.reshape(1,28,28)\n",
    "    x = torch.from_numpy(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "train_set = mnist.MNIST('./data', train=True, transform=data_tf, download=True) # 重新载入数据集，申明定义的数据变换\n",
    "test_set = mnist.MNIST('./data', train=False, transform=data_tf, download=True)\n",
    "\n",
    "a, a_label = train_set[0]\n",
    "print(a.shape)\n",
    "print(a_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: <function data_tf at 0x000000001DDAF400>\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: <function data_tf at 0x000000001DDAF400>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# 使用 pytorch 自带的 DataLoader 定义一个数据迭代器\n",
    "train_data = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_data = DataLoader(test_set, batch_size=128, shuffle=False)\n",
    "print(train_data.dataset)\n",
    "print(test_data.dataset)\n",
    "a, a_label = next(iter(train_data))\n",
    "# 打印出一个批次的数据大小\n",
    "print(a.shape)\n",
    "print(a_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "def Random_affine(image,alpha_affine,random_state=None):\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "    shape = image.shape\n",
    "    shape_size = shape[:2]\n",
    "    # Random affine\n",
    "    center_square = np.float32(shape_size) // 2\n",
    "    square_size = min(shape_size) // 3\n",
    "    pts1 = np.float32([center_square + square_size, [center_square[0]+square_size, center_square[1]-square_size], center_square - square_size])\n",
    "    pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "    #print(image.shape)\n",
    "    #print(M)\n",
    "    #print(shape_size[::-1])\n",
    "    image = cupy.asnumpy(image)\n",
    "    image = cv2.warpAffine(image,M,shape_size[::-1],borderValue=-1)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConv,self).__init__()\n",
    "        self.layer1=nn.Sequential(nn.Conv2d(1,5,kernel_size=5,stride=(2,2),padding=(1,1)),nn.BatchNorm2d(5),nn.Sigmoid())#5 13 13\n",
    "        self.layer2=nn.Sequential(nn.Conv2d(5,50,kernel_size=5,stride=(2,2)),nn.BatchNorm2d(50),nn.Sigmoid()) #50 5 5\n",
    "       \n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(5*5*50,100),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(100,10),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x =  self.layer1(x)\n",
    "        x =  self.layer2(x)\n",
    "        x =  x.view(x.size(0),-1)#展平\n",
    "        x =  self.fc(x)\n",
    "        return  x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleConv(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 5, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Sigmoid()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(5, 50, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Sigmoid()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1250, out_features=100, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if(torch.cuda.is_available()):\n",
    "    net=SimpleConv().cuda()\n",
    "    print(\"cuda\")\n",
    "else:\n",
    "    net=SimpleConv()\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "# 定义 loss 函数\n",
    "loss_func  = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), 0.005) # 使用随机梯度下降，学习率 0.1\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def get_acc(output, label):\n",
    "    total = output.shape[0]\n",
    "    _, pred_label = output.max(1)\n",
    "    num_correct = (pred_label == label).sum().item()\n",
    "    return num_correct / total\n",
    "\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, optimizer, criterion,scheduler):\n",
    "    if torch.cuda.is_available():\n",
    "        net = net.cuda()\n",
    "    prev_time = datetime.now()\n",
    "    for epoch in range(num_epochs):\n",
    "        scheduler.step()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        net = net.train()\n",
    "        for im, label in train_data:\n",
    "          new_data=[]\n",
    "          train_list=[]\n",
    "          train_list.append(im)\n",
    "        \n",
    "        \n",
    "          for i in range(im.shape[0]):\n",
    "            if(i%2):\n",
    "              a_data=np.reshape(im[i],(28,28))\n",
    "              affine=Random_affine(a_data,4)\n",
    "              affine=np.reshape(affine,(1,28,28))\n",
    "              new_data.append(affine)\n",
    "\n",
    "            else:     \n",
    "              ela_temp=elastic_transformations(34,4)\n",
    "              elastic=ela_temp(im[i])\n",
    "              elastic=np.array(elastic, dtype='float32')\n",
    "              new_data.append(elastic)\n",
    "                \n",
    "                \n",
    "          new_data=np.array(new_data)\n",
    "          new_data=torch.from_numpy(new_data)\n",
    "          train_list.append(new_data)\n",
    "\n",
    "        \n",
    "          for ims in train_list:\n",
    "            if torch.cuda.is_available():\n",
    "                ims = Variable(ims.cuda())  # (bs, 3, h, w)\n",
    "                label = Variable(label.cuda())  # (bs, h, w)\n",
    "            else:\n",
    "                ims = Variable(ims)\n",
    "                label = Variable(label)\n",
    "            # forward\n",
    "            #print(ims.shape)\n",
    "            output = net(ims)\n",
    "            loss = criterion(output, label)\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += get_acc(output, label)\n",
    "\n",
    "        cur_time = datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_loss = 0\n",
    "            valid_acc = 0\n",
    "            net = net.eval()\n",
    "            for im, label in valid_data:\n",
    "                if torch.cuda.is_available():\n",
    "                    im = Variable(im.cuda(), volatile=True)\n",
    "                    label = Variable(label.cuda(), volatile=True)\n",
    "                else:\n",
    "                    im = Variable(im, volatile=True)\n",
    "                    label = Variable(label, volatile=True)\n",
    "                output = net(im)\n",
    "                loss = criterion(output, label)\n",
    "                valid_loss += loss.item()\n",
    "                valid_acc += get_acc(output, label)\n",
    "            epoch_str = (\n",
    "                \"Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, \"\n",
    "                % (epoch, train_loss / len(train_data),\n",
    "                   train_acc / len(train_data), valid_loss / len(valid_data),\n",
    "                   valid_acc / len(valid_data)))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Train Loss: %f, Train Acc: %f, \" %\n",
    "                         (epoch, train_loss / len(train_data),\n",
    "                          train_acc / len(train_data)))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将预训练模型载入\n",
    "net.load_state_dict(torch.load('./elastic_affine.pth',map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:72: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:73: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 0.147779, Train Acc: 1.956940, Valid Loss: 0.025535, Valid Acc: 0.991693, Time 00:02:04\n",
      "Epoch 1. Train Loss: 0.145682, Train Acc: 1.956706, Valid Loss: 0.025778, Valid Acc: 0.991594, Time 00:02:07\n",
      "Epoch 2. Train Loss: 0.147109, Train Acc: 1.955740, Valid Loss: 0.025483, Valid Acc: 0.991891, Time 00:02:04\n",
      "Epoch 3. Train Loss: 0.146289, Train Acc: 1.956440, Valid Loss: 0.025595, Valid Acc: 0.991990, Time 00:02:05\n",
      "Epoch 4. Train Loss: 0.145795, Train Acc: 1.956040, Valid Loss: 0.025592, Valid Acc: 0.991594, Time 00:02:07\n",
      "Epoch 5. Train Loss: 0.148365, Train Acc: 1.954724, Valid Loss: 0.025894, Valid Acc: 0.991693, Time 00:02:00\n",
      "Epoch 6. Train Loss: 0.148156, Train Acc: 1.955241, Valid Loss: 0.025382, Valid Acc: 0.991693, Time 00:02:01\n",
      "Epoch 7. Train Loss: 0.147419, Train Acc: 1.955324, Valid Loss: 0.025853, Valid Acc: 0.991594, Time 00:02:00\n",
      "Epoch 8. Train Loss: 0.145911, Train Acc: 1.955890, Valid Loss: 0.025967, Valid Acc: 0.991594, Time 00:02:02\n",
      "Epoch 9. Train Loss: 0.146608, Train Acc: 1.955440, Valid Loss: 0.026271, Valid Acc: 0.991396, Time 00:02:02\n",
      "Epoch 10. Train Loss: 0.146625, Train Acc: 1.954891, Valid Loss: 0.025764, Valid Acc: 0.991199, Time 00:02:01\n",
      "Epoch 11. Train Loss: 0.145143, Train Acc: 1.956640, Valid Loss: 0.025730, Valid Acc: 0.991495, Time 00:02:02\n",
      "Epoch 12. Train Loss: 0.146818, Train Acc: 1.955557, Valid Loss: 0.025847, Valid Acc: 0.991693, Time 00:02:05\n",
      "Epoch 13. Train Loss: 0.144233, Train Acc: 1.957439, Valid Loss: 0.025774, Valid Acc: 0.991199, Time 00:02:04\n",
      "Epoch 14. Train Loss: 0.149391, Train Acc: 1.955157, Valid Loss: 0.026356, Valid Acc: 0.991990, Time 00:02:03\n",
      "Epoch 15. Train Loss: 0.147349, Train Acc: 1.955607, Valid Loss: 0.025529, Valid Acc: 0.991396, Time 00:02:06\n",
      "Epoch 16. Train Loss: 0.145926, Train Acc: 1.956340, Valid Loss: 0.025941, Valid Acc: 0.991990, Time 00:02:05\n",
      "Epoch 17. Train Loss: 0.144194, Train Acc: 1.957872, Valid Loss: 0.025415, Valid Acc: 0.991693, Time 00:02:05\n",
      "Epoch 18. Train Loss: 0.147220, Train Acc: 1.955224, Valid Loss: 0.025974, Valid Acc: 0.991594, Time 00:02:09\n",
      "Epoch 19. Train Loss: 0.146860, Train Acc: 1.956373, Valid Loss: 0.026083, Valid Acc: 0.991792, Time 00:02:15\n",
      "Epoch 20. Train Loss: 0.143229, Train Acc: 1.958039, Valid Loss: 0.025542, Valid Acc: 0.991297, Time 00:02:13\n",
      "Epoch 21. Train Loss: 0.147921, Train Acc: 1.955890, Valid Loss: 0.025703, Valid Acc: 0.991594, Time 00:02:04\n",
      "Epoch 22. Train Loss: 0.146075, Train Acc: 1.956823, Valid Loss: 0.026037, Valid Acc: 0.991396, Time 00:02:06\n",
      "Epoch 23. Train Loss: 0.144816, Train Acc: 1.956457, Valid Loss: 0.025349, Valid Acc: 0.991495, Time 00:02:10\n",
      "Epoch 24. Train Loss: 0.145661, Train Acc: 1.956507, Valid Loss: 0.025927, Valid Acc: 0.991693, Time 00:02:06\n",
      "Epoch 25. Train Loss: 0.145853, Train Acc: 1.955740, Valid Loss: 0.026292, Valid Acc: 0.991693, Time 00:02:07\n",
      "Epoch 26. Train Loss: 0.144134, Train Acc: 1.956606, Valid Loss: 0.026085, Valid Acc: 0.992089, Time 00:02:04\n",
      "Epoch 27. Train Loss: 0.146604, Train Acc: 1.955207, Valid Loss: 0.025664, Valid Acc: 0.991495, Time 00:02:09\n",
      "Epoch 28. Train Loss: 0.148233, Train Acc: 1.955390, Valid Loss: 0.025536, Valid Acc: 0.991792, Time 00:02:10\n",
      "Epoch 29. Train Loss: 0.144065, Train Acc: 1.957656, Valid Loss: 0.024896, Valid Acc: 0.991891, Time 00:02:06\n",
      "Epoch 30. Train Loss: 0.146570, Train Acc: 1.955557, Valid Loss: 0.025910, Valid Acc: 0.991297, Time 00:02:08\n",
      "Epoch 31. Train Loss: 0.144811, Train Acc: 1.956207, Valid Loss: 0.026016, Valid Acc: 0.991396, Time 00:02:05\n",
      "Epoch 32. Train Loss: 0.149207, Train Acc: 1.955390, Valid Loss: 0.026151, Valid Acc: 0.991594, Time 00:02:16\n",
      "Epoch 33. Train Loss: 0.149558, Train Acc: 1.955340, Valid Loss: 0.024995, Valid Acc: 0.992089, Time 00:02:23\n",
      "Epoch 34. Train Loss: 0.146081, Train Acc: 1.956090, Valid Loss: 0.025561, Valid Acc: 0.992286, Time 00:02:19\n",
      "Epoch 35. Train Loss: 0.145131, Train Acc: 1.956840, Valid Loss: 0.025787, Valid Acc: 0.991792, Time 00:02:24\n",
      "Epoch 36. Train Loss: 0.145183, Train Acc: 1.956690, Valid Loss: 0.026142, Valid Acc: 0.991495, Time 00:02:21\n",
      "Epoch 37. Train Loss: 0.146512, Train Acc: 1.956273, Valid Loss: 0.026289, Valid Acc: 0.991495, Time 00:02:20\n",
      "Epoch 38. Train Loss: 0.148019, Train Acc: 1.955007, Valid Loss: 0.026167, Valid Acc: 0.991792, Time 00:02:19\n",
      "Epoch 39. Train Loss: 0.145551, Train Acc: 1.956640, Valid Loss: 0.026250, Valid Acc: 0.991693, Time 00:02:22\n",
      "Epoch 40. Train Loss: 0.147293, Train Acc: 1.955740, Valid Loss: 0.025726, Valid Acc: 0.991594, Time 00:02:26\n",
      "Epoch 41. Train Loss: 0.145615, Train Acc: 1.956423, Valid Loss: 0.026778, Valid Acc: 0.991594, Time 00:02:22\n",
      "Epoch 42. Train Loss: 0.144396, Train Acc: 1.956806, Valid Loss: 0.025516, Valid Acc: 0.991693, Time 00:02:22\n",
      "Epoch 43. Train Loss: 0.144431, Train Acc: 1.956790, Valid Loss: 0.025405, Valid Acc: 0.991693, Time 00:02:22\n",
      "Epoch 44. Train Loss: 0.147205, Train Acc: 1.956373, Valid Loss: 0.026024, Valid Acc: 0.991495, Time 00:02:22\n",
      "Epoch 45. Train Loss: 0.144453, Train Acc: 1.957406, Valid Loss: 0.025781, Valid Acc: 0.991891, Time 00:02:22\n",
      "Epoch 46. Train Loss: 0.144504, Train Acc: 1.957339, Valid Loss: 0.025531, Valid Acc: 0.991594, Time 00:02:19\n",
      "Epoch 47. Train Loss: 0.144118, Train Acc: 1.956540, Valid Loss: 0.026260, Valid Acc: 0.991495, Time 00:02:25\n",
      "Epoch 48. Train Loss: 0.146257, Train Acc: 1.956157, Valid Loss: 0.025334, Valid Acc: 0.991594, Time 00:02:29\n",
      "Epoch 49. Train Loss: 0.145714, Train Acc: 1.956373, Valid Loss: 0.026514, Valid Acc: 0.991100, Time 00:02:25\n",
      "Epoch 50. Train Loss: 0.142351, Train Acc: 1.956906, Valid Loss: 0.025849, Valid Acc: 0.991495, Time 00:02:25\n",
      "Epoch 51. Train Loss: 0.145485, Train Acc: 1.956557, Valid Loss: 0.025615, Valid Acc: 0.991495, Time 00:02:25\n",
      "Epoch 52. Train Loss: 0.144457, Train Acc: 1.957023, Valid Loss: 0.025035, Valid Acc: 0.991990, Time 00:02:23\n",
      "Epoch 53. Train Loss: 0.146023, Train Acc: 1.955874, Valid Loss: 0.026087, Valid Acc: 0.991199, Time 00:02:25\n",
      "Epoch 54. Train Loss: 0.145647, Train Acc: 1.955924, Valid Loss: 0.025482, Valid Acc: 0.991891, Time 00:02:23\n",
      "Epoch 55. Train Loss: 0.147365, Train Acc: 1.955374, Valid Loss: 0.025727, Valid Acc: 0.991891, Time 00:02:21\n",
      "Epoch 56. Train Loss: 0.144859, Train Acc: 1.955957, Valid Loss: 0.025773, Valid Acc: 0.991594, Time 00:02:24\n",
      "Epoch 57. Train Loss: 0.147086, Train Acc: 1.955507, Valid Loss: 0.025960, Valid Acc: 0.992089, Time 00:02:16\n",
      "Epoch 58. Train Loss: 0.142624, Train Acc: 1.957489, Valid Loss: 0.025943, Valid Acc: 0.991693, Time 00:02:05\n",
      "Epoch 59. Train Loss: 0.143802, Train Acc: 1.957289, Valid Loss: 0.026310, Valid Acc: 0.991693, Time 00:02:08\n",
      "Epoch 60. Train Loss: 0.143986, Train Acc: 1.956590, Valid Loss: 0.026297, Valid Acc: 0.991693, Time 00:02:07\n",
      "Epoch 61. Train Loss: 0.145399, Train Acc: 1.956190, Valid Loss: 0.025957, Valid Acc: 0.991495, Time 00:02:07\n",
      "Epoch 62. Train Loss: 0.146233, Train Acc: 1.955940, Valid Loss: 0.025582, Valid Acc: 0.991594, Time 00:02:08\n",
      "Epoch 63. Train Loss: 0.145374, Train Acc: 1.957189, Valid Loss: 0.026296, Valid Acc: 0.991495, Time 00:02:10\n",
      "Epoch 64. Train Loss: 0.144142, Train Acc: 1.956856, Valid Loss: 0.026645, Valid Acc: 0.991297, Time 00:02:12\n",
      "Epoch 65. Train Loss: 0.142219, Train Acc: 1.956573, Valid Loss: 0.025359, Valid Acc: 0.991990, Time 00:02:14\n",
      "Epoch 66. Train Loss: 0.144875, Train Acc: 1.956606, Valid Loss: 0.026186, Valid Acc: 0.991396, Time 00:02:09\n",
      "Epoch 67. Train Loss: 0.142886, Train Acc: 1.957673, Valid Loss: 0.025602, Valid Acc: 0.991891, Time 00:02:12\n",
      "Epoch 68. Train Loss: 0.145095, Train Acc: 1.956240, Valid Loss: 0.025973, Valid Acc: 0.991693, Time 00:02:06\n",
      "Epoch 69. Train Loss: 0.145730, Train Acc: 1.956423, Valid Loss: 0.026391, Valid Acc: 0.991001, Time 00:02:07\n",
      "Epoch 70. Train Loss: 0.144268, Train Acc: 1.956073, Valid Loss: 0.025441, Valid Acc: 0.992188, Time 00:02:00\n",
      "Epoch 71. Train Loss: 0.144847, Train Acc: 1.956806, Valid Loss: 0.025700, Valid Acc: 0.991792, Time 00:02:11\n",
      "Epoch 72. Train Loss: 0.146097, Train Acc: 1.956207, Valid Loss: 0.025705, Valid Acc: 0.991495, Time 00:02:13\n",
      "Epoch 73. Train Loss: 0.145820, Train Acc: 1.956523, Valid Loss: 0.025801, Valid Acc: 0.991594, Time 00:02:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74. Train Loss: 0.146512, Train Acc: 1.955624, Valid Loss: 0.026365, Valid Acc: 0.991693, Time 00:02:11\n",
      "Epoch 75. Train Loss: 0.146870, Train Acc: 1.955490, Valid Loss: 0.025108, Valid Acc: 0.991792, Time 00:02:03\n",
      "Epoch 76. Train Loss: 0.144169, Train Acc: 1.957439, Valid Loss: 0.026180, Valid Acc: 0.991594, Time 00:02:08\n",
      "Epoch 77. Train Loss: 0.148246, Train Acc: 1.954907, Valid Loss: 0.025221, Valid Acc: 0.991594, Time 00:02:11\n",
      "Epoch 78. Train Loss: 0.144555, Train Acc: 1.956723, Valid Loss: 0.025489, Valid Acc: 0.991396, Time 00:02:14\n",
      "Epoch 79. Train Loss: 0.147424, Train Acc: 1.955440, Valid Loss: 0.025239, Valid Acc: 0.992089, Time 00:02:12\n",
      "Epoch 80. Train Loss: 0.143946, Train Acc: 1.956507, Valid Loss: 0.025689, Valid Acc: 0.992286, Time 00:02:09\n",
      "Epoch 81. Train Loss: 0.144559, Train Acc: 1.956806, Valid Loss: 0.025625, Valid Acc: 0.991594, Time 00:02:07\n",
      "Epoch 82. Train Loss: 0.140856, Train Acc: 1.958189, Valid Loss: 0.025347, Valid Acc: 0.991792, Time 00:02:16\n",
      "Epoch 83. Train Loss: 0.145697, Train Acc: 1.956906, Valid Loss: 0.025461, Valid Acc: 0.991594, Time 00:02:15\n",
      "Epoch 84. Train Loss: 0.145189, Train Acc: 1.955407, Valid Loss: 0.025822, Valid Acc: 0.992089, Time 00:02:20\n",
      "Epoch 85. Train Loss: 0.145962, Train Acc: 1.956290, Valid Loss: 0.024807, Valid Acc: 0.991495, Time 00:02:15\n",
      "Epoch 86. Train Loss: 0.142758, Train Acc: 1.957189, Valid Loss: 0.025709, Valid Acc: 0.991199, Time 00:02:07\n",
      "Epoch 87. Train Loss: 0.145215, Train Acc: 1.956956, Valid Loss: 0.025525, Valid Acc: 0.992089, Time 00:02:05\n",
      "Epoch 88. Train Loss: 0.145371, Train Acc: 1.955924, Valid Loss: 0.026015, Valid Acc: 0.991792, Time 00:02:03\n",
      "Epoch 89. Train Loss: 0.139511, Train Acc: 1.957706, Valid Loss: 0.026332, Valid Acc: 0.991396, Time 00:02:02\n",
      "Epoch 90. Train Loss: 0.143268, Train Acc: 1.956507, Valid Loss: 0.026003, Valid Acc: 0.991891, Time 00:02:03\n",
      "Epoch 91. Train Loss: 0.142486, Train Acc: 1.957073, Valid Loss: 0.025483, Valid Acc: 0.991891, Time 00:02:09\n",
      "Epoch 92. Train Loss: 0.144451, Train Acc: 1.956656, Valid Loss: 0.026298, Valid Acc: 0.991990, Time 00:02:06\n",
      "Epoch 93. Train Loss: 0.142723, Train Acc: 1.957156, Valid Loss: 0.025846, Valid Acc: 0.991693, Time 00:02:22\n",
      "Epoch 94. Train Loss: 0.140799, Train Acc: 1.957523, Valid Loss: 0.026048, Valid Acc: 0.991990, Time 00:02:12\n",
      "Epoch 95. Train Loss: 0.145001, Train Acc: 1.956740, Valid Loss: 0.025212, Valid Acc: 0.991792, Time 00:02:04\n",
      "Epoch 96. Train Loss: 0.144812, Train Acc: 1.956440, Valid Loss: 0.025104, Valid Acc: 0.991792, Time 00:02:03\n",
      "Epoch 97. Train Loss: 0.143504, Train Acc: 1.957239, Valid Loss: 0.025634, Valid Acc: 0.991693, Time 00:02:08\n",
      "Epoch 98. Train Loss: 0.143709, Train Acc: 1.957256, Valid Loss: 0.026028, Valid Acc: 0.991297, Time 00:02:11\n",
      "Epoch 99. Train Loss: 0.142250, Train Acc: 1.957406, Valid Loss: 0.025440, Valid Acc: 0.991495, Time 00:02:13\n",
      "Epoch 100. Train Loss: 0.138838, Train Acc: 1.958472, Valid Loss: 0.025461, Valid Acc: 0.991693, Time 00:02:19\n",
      "Epoch 101. Train Loss: 0.144710, Train Acc: 1.957373, Valid Loss: 0.025650, Valid Acc: 0.991594, Time 00:02:07\n",
      "Epoch 102. Train Loss: 0.142359, Train Acc: 1.957723, Valid Loss: 0.025436, Valid Acc: 0.991891, Time 00:02:09\n",
      "Epoch 103. Train Loss: 0.141447, Train Acc: 1.957822, Valid Loss: 0.025309, Valid Acc: 0.991792, Time 00:02:35\n",
      "Epoch 104. Train Loss: 0.145693, Train Acc: 1.957023, Valid Loss: 0.025384, Valid Acc: 0.991792, Time 00:02:20\n",
      "Epoch 105. Train Loss: 0.143763, Train Acc: 1.956723, Valid Loss: 0.024946, Valid Acc: 0.991990, Time 00:02:26\n",
      "Epoch 106. Train Loss: 0.140408, Train Acc: 1.957173, Valid Loss: 0.025575, Valid Acc: 0.991594, Time 00:02:41\n",
      "Epoch 107. Train Loss: 0.140531, Train Acc: 1.956940, Valid Loss: 0.025437, Valid Acc: 0.991891, Time 00:02:55\n",
      "Epoch 108. Train Loss: 0.144960, Train Acc: 1.956623, Valid Loss: 0.026043, Valid Acc: 0.991594, Time 00:03:01\n"
     ]
    }
   ],
   "source": [
    "train(net, train_data, test_data, 200, optimizer, loss_func,scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存模型\n",
    "model_path='./elastic_affine.pth'\n",
    "torch.save(net.state_dict(),model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
