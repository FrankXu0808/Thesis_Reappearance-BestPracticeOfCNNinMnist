{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import cv2\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import mnist # 导入 pytorch 内置的 mnist 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Elastic_transform(image, alpha, sigma):\n",
    "    shape = image.shape\n",
    "    shape_size = shape[:2]\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "    dz = np.zeros_like(dx)\n",
    "\n",
    "    x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n",
    "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1)), np.reshape(z, (-1, 1))\n",
    "\n",
    "    return map_coordinates(image, indices, order=1, mode='reflect').reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_transform(image, alpha, sigma, random_state=None):\n",
    "    \"\"\"Elastic deformation of images as described in [Simard2003]_.\n",
    "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
    "       Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
    "       Proc. of the International Conference on Document Analysis and\n",
    "       Recognition, 2003.\n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "\n",
    "    shape = image.shape\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dz = np.zeros_like(dx)\n",
    "\n",
    "    x, y, z = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), np.arange(shape[2]))\n",
    "    print(x.shape)\n",
    "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1)), np.reshape(z, (-1, 1))\n",
    " \n",
    "    distored_image = map_coordinates(image, indices,order=1, mode='reflect')\n",
    "    return distored_image.reshape(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic transform\n",
    "def elastic_transformations(alpha, sigma, rng=np.random.RandomState(42), \n",
    "                            interpolation_order=1):\n",
    "    \"\"\"Returns a function to elastically transform multiple images.\"\"\"\n",
    "    # Good values for:\n",
    "    #   alpha: 2000\n",
    "    #   sigma: between 40 and 60\n",
    "    def _elastic_transform_2D(images):\n",
    "        \"\"\"`images` is a numpy array of shape (K, M, N) of K images of size M*N.\"\"\"\n",
    "        # Take measurements\n",
    "        image_shape = images[0].shape\n",
    "        # Make random fields\n",
    "        dx = rng.uniform(-1, 1, image_shape) * alpha\n",
    "        dy = rng.uniform(-1, 1, image_shape) * alpha\n",
    "        # Smooth dx and dy\n",
    "        sdx = gaussian_filter(dx, sigma=sigma, mode='reflect')\n",
    "        sdy = gaussian_filter(dy, sigma=sigma, mode='reflect')\n",
    "        # Make meshgrid\n",
    "        x, y = np.meshgrid(np.arange(image_shape[1]), np.arange(image_shape[0]))\n",
    "        # Distort meshgrid indices\n",
    "        distorted_indices = (y + sdy).reshape(-1, 1), \\\n",
    "                            (x + sdx).reshape(-1, 1)\n",
    "\n",
    "        # Map cooordinates from image to distorted index set\n",
    "        transformed_images = [map_coordinates(image, distorted_indices, mode='reflect',\n",
    "                                              order=interpolation_order).reshape(image_shape)\n",
    "                              for image in images]\n",
    "        return transformed_images\n",
    "    return _elastic_transform_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用内置函数下载 mnist 数据集\n",
    "train_set = mnist.MNIST('./data', train=True, download=True)\n",
    "test_set = mnist.MNIST('./data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -0.6000,  0.2471,  0.9843,  0.2471, -0.6078, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.6235,  0.8667,  0.9765,  0.9765,  0.9765,  0.8588, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5765,\n",
       "           0.7804,  0.9843,  0.9765,  0.8745,  0.8275,  0.9765, -0.5529,\n",
       "          -0.9529, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -0.9216, -0.5294,  0.7569,\n",
       "           0.9765,  0.9843,  0.9765,  0.5843, -0.3412,  0.9765,  0.9843,\n",
       "          -0.0431, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000,  0.2784,  0.9765,  0.9765,\n",
       "           0.9765,  0.9843,  0.9765,  0.9765, -0.2471,  0.4824,  0.9843,\n",
       "           0.3098, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -0.6000,  0.8667,  0.9843,  0.9843,\n",
       "           0.4902, -0.1059,  0.9843,  0.7882, -0.6314, -0.3804,  1.0000,\n",
       "           0.3176, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -0.6235,  0.8667,  0.9765,  0.9765,  0.4039,\n",
       "          -0.9059, -0.4118, -0.0510, -0.8353, -1.0000, -1.0000,  0.9843,\n",
       "           0.9059, -0.6078, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -0.7020,  0.2941,  0.9843,  0.8275,  0.6314, -0.3412,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9843,\n",
       "           0.9765,  0.2941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.9451,  0.3961,  0.9765,  0.8824, -0.4431, -0.8510, -0.7804,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9843,\n",
       "           0.9765,  0.5294, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.5529,  0.9765,  0.9765, -0.5059, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9843,\n",
       "           0.9765,  0.5294, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           0.5529,  0.9843,  0.4902, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n",
       "           0.9843,  0.5373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4039,\n",
       "           0.9294,  0.9765, -0.1216, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9843,\n",
       "           0.9765,  0.1608, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n",
       "           0.9765,  0.8039, -0.8039, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -0.9451,  0.0588,  0.9843,\n",
       "           0.4588, -0.9059, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n",
       "           0.9765,  0.7490, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -0.9451,  0.0275,  0.9765,  0.7647,\n",
       "          -0.4431, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n",
       "           0.9765,  0.1373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -0.6235,  0.2941,  0.9765,  0.3569, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3255,\n",
       "           0.9843,  0.7647, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -0.1059,  0.8667,  0.9843,  0.2706, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n",
       "           0.9765,  0.9529,  0.1451, -0.6235, -0.7725, -0.3333,  0.3961,\n",
       "           0.7647,  0.9843,  0.7490,  0.3098, -0.5608, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n",
       "           0.9765,  0.9765,  0.9765,  0.7961,  0.6863,  0.9765,  0.9765,\n",
       "           0.9765,  0.5373,  0.0196, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7804,\n",
       "           0.5608,  0.9765,  0.9765,  0.9843,  0.9765,  0.9765,  0.8275,\n",
       "           0.1373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.8039,  0.0039,  0.9765,  0.9843,  0.9765,  0.1059, -0.7098,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_data, a_label = train_set[1]\n",
    "a_data\n",
    "#plt.imshow(a_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "def data_tf(x):\n",
    "    x = np.array(x, dtype='float32') / 255\n",
    "    x = (x - 0.5) / 0.5 # 标准化，这个技巧之后会讲到\n",
    "    x = x.reshape(1,28,28)\n",
    "    x = torch.from_numpy(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "train_set = mnist.MNIST('./data', train=True, transform=data_tf, download=True) # 重新载入数据集，申明定义的数据变换\n",
    "test_set = mnist.MNIST('./data', train=False, transform=data_tf, download=True)\n",
    "\n",
    "a, a_label = train_set[0]\n",
    "print(a.shape)\n",
    "print(a_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: <function data_tf at 0x000000001A5A2C80>\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: <function data_tf at 0x000000001A5A2C80>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# 使用 pytorch 自带的 DataLoader 定义一个数据迭代器\n",
    "train_data = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_data = DataLoader(test_set, batch_size=128, shuffle=False)\n",
    "print(train_data.dataset)\n",
    "print(test_data.dataset)\n",
    "a, a_label = next(iter(train_data))\n",
    "# 打印出一个批次的数据大小\n",
    "print(a.shape)\n",
    "print(a_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "def Random_affine(image,alpha_affine,random_state=None):\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "    shape = image.shape\n",
    "    shape_size = shape[:2]\n",
    "    # Random affine\n",
    "    center_square = np.float32(shape_size) // 2\n",
    "    square_size = min(shape_size) // 3\n",
    "    pts1 = np.float32([center_square + square_size, [center_square[0]+square_size, center_square[1]-square_size], center_square - square_size])\n",
    "    pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "    print(image.shape)\n",
    "    print(M)\n",
    "    print(shape_size[::-1])\n",
    "    image = cupy.asnumpy(image)\n",
    "    image = cv2.warpAffine(image,M,shape_size[::-1],borderValue=-1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n",
      "[[ 1.11542327  0.2058542  -5.24250268]\n",
      " [-0.10298035  1.09621199  1.79910869]]\n",
      "torch.Size([28, 28])\n",
      "[[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -0.96905637 -0.9880515  -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -0.976394\n",
      "  -0.61511946  0.41522673 -0.2912454  -0.87573534 -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -0.36619177  0.27729017 -0.1824142  -0.8279412  -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-0.9658088  -0.8913603  -0.9255515  -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -0.9941176\n",
      "  -0.31222427  0.33860296  0.10955885 -0.765625   -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-0.7112745  -0.40836394 -0.5171415  -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -0.94636947\n",
      "  -0.19142157  0.5732997   0.11734071 -0.9200368  -0.9981618  -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-0.5882353   0.06185663  0.03872552 -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -0.7586397\n",
      "   0.04227944  0.8313572   0.04664524 -0.931924   -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-0.6089461   0.5778799   0.23848045 -0.8801471  -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -0.6356617\n",
      "   0.5338542   0.99215686 -0.10030638 -0.84842986 -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-0.83722425  0.6834406   0.3359682  -0.68039215 -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -0.83681065\n",
      "   0.55248165  0.93363976 -0.17634039 -0.8592065  -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-0.7991728   0.5552849   0.46531865 -0.44068626 -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -0.9948912  -0.8365196\n",
      "   0.47872245  0.9125766  -0.04742646 -0.8899816  -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-0.50559133  0.52333033  0.60161614 -0.24093135 -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -0.14411764\n",
      "   0.7081189   0.8125     -0.02227327 -0.9295343  -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-0.37306982  0.5099954   0.6573223  -0.19926469 -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -0.99699754 -0.9453125  -0.64460784 -0.19522057  0.51140475\n",
      "   0.922886    0.5737592  -0.45216757 -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-0.45441175  0.47990197  0.7609988  -0.347189   -0.97830117 -1.\n",
      "  -1.         -1.         -1.         -0.9818321  -0.90955883 -0.7871323\n",
      "  -0.7058823  -0.37324598 -0.11942402  0.16997549  0.38338697  0.60698533\n",
      "   0.92781866  0.6784161  -0.6824449  -0.98039216 -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-0.6492647   0.36348042  0.94784003 -0.28503367 -0.8696078  -1.\n",
      "  -1.         -0.8790441  -0.68483454 -0.5624234  -0.06960784  0.19401047\n",
      "   0.2371324   0.5778187   0.5807292   0.30024508 -0.08725488 -0.05906864\n",
      "   0.8636642   0.9647058  -0.7625919  -0.99356616 -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-0.8835018   0.23602946  0.8955882   0.27683824 -0.15615809 -0.23897058\n",
      "  -0.08676469  0.16767003  0.53147215  0.6929687   0.86030173  0.8717831\n",
      "   0.55238974  0.10556069 -0.263174   -0.61547184 -0.82178307 -0.4627144\n",
      "   0.87469363  0.83195466 -0.49609375 -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -0.07398893  0.5955883   0.75528497  0.6471967   0.5352941\n",
      "   0.51323533  0.5143842   0.6117647   0.16813728 -0.16188729 -0.61691177\n",
      "  -0.9463235  -1.         -1.         -1.         -0.97038144 -0.09641543\n",
      "   0.8117341   0.57106316 -0.42310047 -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -0.95174634 -0.5196614   0.0133119   0.12794119 -0.00220585\n",
      "  -0.08897057 -0.21911763 -0.34926468 -0.61695004 -0.82092524 -0.94895834\n",
      "  -1.         -1.         -1.         -1.         -1.         -0.00588232\n",
      "   0.74215686  0.35882354 -0.5482843  -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -0.9671875  -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -0.21299018\n",
      "   0.63799024  0.4610294  -0.6507353  -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -0.420098\n",
      "   0.5342065   0.7157476  -0.49586397 -0.9652574  -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -0.66862744\n",
      "   0.40977332  0.95965075 -0.1414139  -0.8377604  -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -0.87573534\n",
      "   0.28406867  0.8879902   0.09417894 -0.6862132  -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "   0.07696082  0.78382355  0.54656863 -0.325      -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -0.13014705  0.6830423   0.67456347 -0.13749996 -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -0.54641545  0.2853554   0.79540443  0.05000004 -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -0.8897059  -0.5997243  -0.3929228  -0.7320849  -0.9707031  -1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -1.         -1.         -1.         -1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b941470>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a_data, a_label = train_set[2]\n",
    "a_data=np.reshape(a_data,(28,28))\n",
    "a_data=Random_affine(a_data,4)\n",
    "print(a_data)\n",
    "\n",
    "plt.imshow(a_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b98b2b0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b_data, b_label = train_set[2]\n",
    "b_data.shape\n",
    "b_data=np.reshape(b_data,(28,28))\n",
    "b_data\n",
    "plt.imshow(b_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1dc204e0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a_data, a_label = train_set[2]\n",
    "a_data\n",
    "k=elastic_transformations(34,4)\n",
    "a_data=k(a_data)\n",
    "#a_data=elastic_transform(a_data,34,4)\n",
    "a_data=np.array(a_data, dtype='float32')\n",
    "a_data.shape\n",
    "a_data=np.reshape(a_data,(28,28))\n",
    "\n",
    "print()\n",
    "plt.imshow(a_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected Ptr<cv::UMat> for argument '%s'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-13c9bc065109>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRandom_affine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m34\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0ma_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-43b2c751e65b>\u001b[0m in \u001b[0;36mRandom_affine\u001b[1;34m(image, alpha_affine, random_state)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mpts2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpts1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0malpha_affine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha_affine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpts1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetAffineTransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpts1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpts2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarpAffine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mborderMode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBORDER_REFLECT_101\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected Ptr<cv::UMat> for argument '%s'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConv,self).__init__()\n",
    "        self.layer1=nn.Sequential(nn.Conv2d(1,5,kernel_size=5,stride=(2,2),padding=(1,1)),nn.BatchNorm2d(5),nn.Sigmoid())#5 13 13\n",
    "        self.layer2=nn.Sequential(nn.Conv2d(5,50,kernel_size=5,stride=(2,2)),nn.BatchNorm2d(50),nn.Sigmoid()) #50 5 5\n",
    "       \n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(5*5*50,100),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(100,10),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x =  self.layer1(x)\n",
    "        x =  self.layer2(x)\n",
    "        x =  x.view(x.size(0),-1)#展平\n",
    "        x =  self.fc(x)\n",
    "        return  x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from utils import train\n",
    "print(torch.cuda.is_available())\n",
    "if(torch.cuda.is_available()):\n",
    "    net=SimpleConv().cuda()\n",
    "    print(\"cuda\")\n",
    "else:\n",
    "    net=SimpleConv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "# 定义 loss 函数\n",
    "loss_func  = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), 0.005) # 使用随机梯度下降，学习率 0.1\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "E:\\Python file\\Best Practice of CNN in Minst\\utils.py:53: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  im = Variable(im.cuda(), volatile=True)\n",
      "E:\\Python file\\Best Practice of CNN in Minst\\utils.py:54: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  label = Variable(label.cuda(), volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 2.252778, Train Acc: 0.260928, Valid Loss: 2.186798, Valid Acc: 0.549941, Time 00:00:43\n",
      "Epoch 1. Train Loss: 2.082880, Train Acc: 0.577509, Valid Loss: 1.938608, Valid Acc: 0.647547, Time 00:00:31\n",
      "Epoch 2. Train Loss: 1.751771, Train Acc: 0.672225, Valid Loss: 1.529853, Valid Acc: 0.718552, Time 00:00:31\n",
      "Epoch 3. Train Loss: 1.346981, Train Acc: 0.734708, Valid Loss: 1.158583, Valid Acc: 0.775020, Time 00:00:31\n",
      "Epoch 4. Train Loss: 1.044352, Train Acc: 0.784298, Valid Loss: 0.918491, Valid Acc: 0.812797, Time 00:00:30\n",
      "Epoch 5. Train Loss: 0.854692, Train Acc: 0.815082, Valid Loss: 0.768720, Valid Acc: 0.834157, Time 00:00:31\n",
      "Epoch 6. Train Loss: 0.731896, Train Acc: 0.835987, Valid Loss: 0.665838, Valid Acc: 0.849486, Time 00:00:33\n",
      "Epoch 7. Train Loss: 0.646276, Train Acc: 0.850730, Valid Loss: 0.594022, Valid Acc: 0.860463, Time 00:00:33\n",
      "Epoch 8. Train Loss: 0.583972, Train Acc: 0.860541, Valid Loss: 0.540898, Valid Acc: 0.869956, Time 00:00:33\n",
      "Epoch 9. Train Loss: 0.536507, Train Acc: 0.868937, Valid Loss: 0.496868, Valid Acc: 0.881824, Time 00:00:31\n",
      "Epoch 10. Train Loss: 0.499171, Train Acc: 0.875217, Valid Loss: 0.463767, Valid Acc: 0.887658, Time 00:00:31\n",
      "Epoch 11. Train Loss: 0.469015, Train Acc: 0.881380, Valid Loss: 0.435869, Valid Acc: 0.890922, Time 00:00:32\n",
      "Epoch 12. Train Loss: 0.443996, Train Acc: 0.885078, Valid Loss: 0.412856, Valid Acc: 0.894877, Time 00:00:33\n",
      "Epoch 13. Train Loss: 0.422993, Train Acc: 0.889259, Valid Loss: 0.395476, Valid Acc: 0.900218, Time 00:00:32\n",
      "Epoch 14. Train Loss: 0.404723, Train Acc: 0.892857, Valid Loss: 0.377720, Valid Acc: 0.903975, Time 00:00:32\n",
      "Epoch 15. Train Loss: 0.388632, Train Acc: 0.896489, Valid Loss: 0.362590, Valid Acc: 0.906843, Time 00:00:33\n",
      "Epoch 16. Train Loss: 0.374293, Train Acc: 0.899487, Valid Loss: 0.348219, Valid Acc: 0.909217, Time 00:00:33\n",
      "Epoch 17. Train Loss: 0.360956, Train Acc: 0.902802, Valid Loss: 0.336907, Valid Acc: 0.912184, Time 00:00:32\n",
      "Epoch 18. Train Loss: 0.348695, Train Acc: 0.905284, Valid Loss: 0.324975, Valid Acc: 0.915843, Time 00:00:33\n",
      "Epoch 19. Train Loss: 0.337353, Train Acc: 0.908416, Valid Loss: 0.312757, Valid Acc: 0.916139, Time 00:00:34\n",
      "Epoch 20. Train Loss: 0.326230, Train Acc: 0.910764, Valid Loss: 0.303347, Valid Acc: 0.920985, Time 00:00:33\n",
      "Epoch 21. Train Loss: 0.315797, Train Acc: 0.913896, Valid Loss: 0.295829, Valid Acc: 0.921381, Time 00:00:33\n",
      "Epoch 22. Train Loss: 0.305815, Train Acc: 0.916195, Valid Loss: 0.283685, Valid Acc: 0.924743, Time 00:00:33\n",
      "Epoch 23. Train Loss: 0.295928, Train Acc: 0.919376, Valid Loss: 0.273966, Valid Acc: 0.927017, Time 00:00:33\n",
      "Epoch 24. Train Loss: 0.286310, Train Acc: 0.921742, Valid Loss: 0.264482, Valid Acc: 0.928797, Time 00:00:32\n",
      "Epoch 25. Train Loss: 0.276982, Train Acc: 0.924157, Valid Loss: 0.256391, Valid Acc: 0.930578, Time 00:00:32\n",
      "Epoch 26. Train Loss: 0.268015, Train Acc: 0.926872, Valid Loss: 0.247365, Valid Acc: 0.933445, Time 00:00:34\n",
      "Epoch 27. Train Loss: 0.259184, Train Acc: 0.929154, Valid Loss: 0.238872, Valid Acc: 0.935324, Time 00:00:33\n",
      "Epoch 28. Train Loss: 0.250510, Train Acc: 0.931903, Valid Loss: 0.231076, Valid Acc: 0.937203, Time 00:00:34\n",
      "Epoch 29. Train Loss: 0.242075, Train Acc: 0.934218, Valid Loss: 0.223656, Valid Acc: 0.939280, Time 00:00:32\n",
      "Epoch 30. Train Loss: 0.233950, Train Acc: 0.936267, Valid Loss: 0.215097, Valid Acc: 0.942445, Time 00:00:33\n",
      "Epoch 31. Train Loss: 0.226242, Train Acc: 0.938799, Valid Loss: 0.206362, Valid Acc: 0.943730, Time 00:00:32\n",
      "Epoch 32. Train Loss: 0.218584, Train Acc: 0.940648, Valid Loss: 0.200580, Valid Acc: 0.946203, Time 00:00:33\n",
      "Epoch 33. Train Loss: 0.211526, Train Acc: 0.943214, Valid Loss: 0.194962, Valid Acc: 0.948180, Time 00:00:32\n",
      "Epoch 34. Train Loss: 0.204692, Train Acc: 0.944713, Valid Loss: 0.187184, Valid Acc: 0.949268, Time 00:00:33\n",
      "Epoch 35. Train Loss: 0.198347, Train Acc: 0.946479, Valid Loss: 0.187091, Valid Acc: 0.949367, Time 00:00:33\n",
      "Epoch 36. Train Loss: 0.192326, Train Acc: 0.948544, Valid Loss: 0.177862, Valid Acc: 0.951839, Time 00:00:33\n",
      "Epoch 37. Train Loss: 0.186655, Train Acc: 0.949460, Valid Loss: 0.172514, Valid Acc: 0.952532, Time 00:00:32\n",
      "Epoch 38. Train Loss: 0.181413, Train Acc: 0.950876, Valid Loss: 0.165185, Valid Acc: 0.955004, Time 00:00:34\n",
      "Epoch 39. Train Loss: 0.176192, Train Acc: 0.952759, Valid Loss: 0.165223, Valid Acc: 0.954411, Time 00:00:34\n",
      "Epoch 40. Train Loss: 0.171085, Train Acc: 0.953825, Valid Loss: 0.157883, Valid Acc: 0.955894, Time 00:00:33\n",
      "Epoch 41. Train Loss: 0.166475, Train Acc: 0.955041, Valid Loss: 0.151996, Valid Acc: 0.957278, Time 00:00:33\n",
      "Epoch 42. Train Loss: 0.162112, Train Acc: 0.956090, Valid Loss: 0.146577, Valid Acc: 0.959059, Time 00:00:34\n",
      "Epoch 43. Train Loss: 0.157908, Train Acc: 0.957173, Valid Loss: 0.143446, Valid Acc: 0.959949, Time 00:00:34\n",
      "Epoch 44. Train Loss: 0.153901, Train Acc: 0.958489, Valid Loss: 0.139996, Valid Acc: 0.959850, Time 00:00:34\n",
      "Epoch 45. Train Loss: 0.150077, Train Acc: 0.959438, Valid Loss: 0.138052, Valid Acc: 0.960047, Time 00:00:33\n",
      "Epoch 46. Train Loss: 0.146452, Train Acc: 0.960904, Valid Loss: 0.139261, Valid Acc: 0.959454, Time 00:00:33\n",
      "Epoch 47. Train Loss: 0.142775, Train Acc: 0.961654, Valid Loss: 0.128922, Valid Acc: 0.963904, Time 00:00:37\n",
      "Epoch 48. Train Loss: 0.139352, Train Acc: 0.962437, Valid Loss: 0.125742, Valid Acc: 0.963805, Time 00:00:34\n",
      "Epoch 49. Train Loss: 0.136085, Train Acc: 0.963070, Valid Loss: 0.123540, Valid Acc: 0.965487, Time 00:00:35\n",
      "Epoch 50. Train Loss: 0.132852, Train Acc: 0.964269, Valid Loss: 0.128167, Valid Acc: 0.963014, Time 00:00:35\n",
      "Epoch 51. Train Loss: 0.129762, Train Acc: 0.965152, Valid Loss: 0.116516, Valid Acc: 0.966278, Time 00:00:35\n",
      "Epoch 52. Train Loss: 0.126993, Train Acc: 0.965985, Valid Loss: 0.114050, Valid Acc: 0.967069, Time 00:00:35\n",
      "Epoch 53. Train Loss: 0.124170, Train Acc: 0.966085, Valid Loss: 0.113199, Valid Acc: 0.967267, Time 00:00:35\n",
      "Epoch 54. Train Loss: 0.121271, Train Acc: 0.967251, Valid Loss: 0.109867, Valid Acc: 0.967168, Time 00:00:32\n",
      "Epoch 55. Train Loss: 0.118621, Train Acc: 0.967801, Valid Loss: 0.106152, Valid Acc: 0.968651, Time 00:00:33\n",
      "Epoch 56. Train Loss: 0.116494, Train Acc: 0.968150, Valid Loss: 0.104410, Valid Acc: 0.969442, Time 00:00:37\n",
      "Epoch 57. Train Loss: 0.114236, Train Acc: 0.969133, Valid Loss: 0.102349, Valid Acc: 0.970036, Time 00:00:35\n",
      "Epoch 58. Train Loss: 0.111932, Train Acc: 0.969799, Valid Loss: 0.104693, Valid Acc: 0.969244, Time 00:00:35\n",
      "Epoch 59. Train Loss: 0.110040, Train Acc: 0.969966, Valid Loss: 0.100301, Valid Acc: 0.970530, Time 00:00:35\n",
      "Epoch 60. Train Loss: 0.108161, Train Acc: 0.970599, Valid Loss: 0.108195, Valid Acc: 0.968750, Time 00:00:33\n",
      "Epoch 61. Train Loss: 0.106209, Train Acc: 0.971365, Valid Loss: 0.094368, Valid Acc: 0.973002, Time 00:00:34\n",
      "Epoch 62. Train Loss: 0.104161, Train Acc: 0.971898, Valid Loss: 0.092884, Valid Acc: 0.973398, Time 00:00:34\n",
      "Epoch 63. Train Loss: 0.102491, Train Acc: 0.972015, Valid Loss: 0.091122, Valid Acc: 0.973101, Time 00:00:34\n",
      "Epoch 64. Train Loss: 0.101066, Train Acc: 0.972465, Valid Loss: 0.169280, Valid Acc: 0.952136, Time 00:00:33\n",
      "Epoch 65. Train Loss: 0.099503, Train Acc: 0.972831, Valid Loss: 0.088615, Valid Acc: 0.974288, Time 00:00:32\n",
      "Epoch 66. Train Loss: 0.097949, Train Acc: 0.973414, Valid Loss: 0.089597, Valid Acc: 0.974288, Time 00:00:33\n",
      "Epoch 67. Train Loss: 0.096590, Train Acc: 0.973581, Valid Loss: 0.086575, Valid Acc: 0.975079, Time 00:00:32\n",
      "Epoch 68. Train Loss: 0.095203, Train Acc: 0.973631, Valid Loss: 0.088750, Valid Acc: 0.974288, Time 00:00:32\n",
      "Epoch 69. Train Loss: 0.093847, Train Acc: 0.974247, Valid Loss: 0.088868, Valid Acc: 0.974980, Time 00:00:33\n",
      "Epoch 70. Train Loss: 0.092487, Train Acc: 0.974897, Valid Loss: 0.084365, Valid Acc: 0.975574, Time 00:00:33\n",
      "Epoch 71. Train Loss: 0.091204, Train Acc: 0.975247, Valid Loss: 0.085816, Valid Acc: 0.974486, Time 00:00:33\n",
      "Epoch 72. Train Loss: 0.090315, Train Acc: 0.975396, Valid Loss: 0.083072, Valid Acc: 0.976068, Time 00:00:34\n",
      "Epoch 73. Train Loss: 0.089310, Train Acc: 0.975396, Valid Loss: 0.080278, Valid Acc: 0.976365, Time 00:00:33\n",
      "Epoch 74. Train Loss: 0.087988, Train Acc: 0.975796, Valid Loss: 0.079584, Valid Acc: 0.978244, Time 00:00:32\n",
      "Epoch 75. Train Loss: 0.087304, Train Acc: 0.975630, Valid Loss: 0.079314, Valid Acc: 0.977551, Time 00:00:33\n",
      "Epoch 76. Train Loss: 0.086251, Train Acc: 0.976529, Valid Loss: 0.083078, Valid Acc: 0.975771, Time 00:00:33\n",
      "Epoch 77. Train Loss: 0.085179, Train Acc: 0.976679, Valid Loss: 0.077226, Valid Acc: 0.978046, Time 00:00:33\n",
      "Epoch 78. Train Loss: 0.084195, Train Acc: 0.976812, Valid Loss: 0.076048, Valid Acc: 0.978244, Time 00:00:34\n",
      "Epoch 79. Train Loss: 0.083365, Train Acc: 0.977079, Valid Loss: 0.074747, Valid Acc: 0.978639, Time 00:00:34\n",
      "Epoch 80. Train Loss: 0.082598, Train Acc: 0.977312, Valid Loss: 0.080183, Valid Acc: 0.976859, Time 00:00:33\n",
      "Epoch 81. Train Loss: 0.081631, Train Acc: 0.977562, Valid Loss: 0.073177, Valid Acc: 0.979035, Time 00:00:33\n",
      "Epoch 82. Train Loss: 0.080999, Train Acc: 0.977679, Valid Loss: 0.080687, Valid Acc: 0.976464, Time 00:00:34\n",
      "Epoch 83. Train Loss: 0.080188, Train Acc: 0.978162, Valid Loss: 0.074366, Valid Acc: 0.978046, Time 00:00:32\n",
      "Epoch 84. Train Loss: 0.079138, Train Acc: 0.978412, Valid Loss: 0.070945, Valid Acc: 0.980024, Time 00:00:32\n",
      "Epoch 85. Train Loss: 0.078579, Train Acc: 0.978628, Valid Loss: 0.079488, Valid Acc: 0.976266, Time 00:00:33\n",
      "Epoch 86. Train Loss: 0.077929, Train Acc: 0.978795, Valid Loss: 0.514802, Valid Acc: 0.815763, Time 00:00:34\n",
      "Epoch 87. Train Loss: 0.077037, Train Acc: 0.978695, Valid Loss: 0.073836, Valid Acc: 0.979035, Time 00:00:35\n",
      "Epoch 88. Train Loss: 0.076559, Train Acc: 0.979028, Valid Loss: 0.073737, Valid Acc: 0.979134, Time 00:00:35\n",
      "Epoch 89. Train Loss: 0.075700, Train Acc: 0.979194, Valid Loss: 0.082080, Valid Acc: 0.976464, Time 00:00:33\n",
      "Epoch 90. Train Loss: 0.075140, Train Acc: 0.979311, Valid Loss: 0.073583, Valid Acc: 0.978540, Time 00:00:34\n",
      "Epoch 91. Train Loss: 0.074195, Train Acc: 0.979911, Valid Loss: 0.068482, Valid Acc: 0.980518, Time 00:00:34\n",
      "Epoch 92. Train Loss: 0.073773, Train Acc: 0.980211, Valid Loss: 0.080752, Valid Acc: 0.976464, Time 00:00:34\n",
      "Epoch 93. Train Loss: 0.073228, Train Acc: 0.980094, Valid Loss: 0.070573, Valid Acc: 0.979826, Time 00:00:33\n",
      "Epoch 94. Train Loss: 0.072701, Train Acc: 0.980410, Valid Loss: 0.065947, Valid Acc: 0.981013, Time 00:00:33\n",
      "Epoch 95. Train Loss: 0.072053, Train Acc: 0.980494, Valid Loss: 0.065577, Valid Acc: 0.981507, Time 00:00:35\n",
      "Epoch 96. Train Loss: 0.071447, Train Acc: 0.980660, Valid Loss: 0.070298, Valid Acc: 0.979826, Time 00:00:34\n",
      "Epoch 97. Train Loss: 0.070621, Train Acc: 0.980927, Valid Loss: 0.070745, Valid Acc: 0.978837, Time 00:00:34\n",
      "Epoch 98. Train Loss: 0.070109, Train Acc: 0.981093, Valid Loss: 0.065084, Valid Acc: 0.981606, Time 00:00:34\n",
      "Epoch 99. Train Loss: 0.068525, Train Acc: 0.981310, Valid Loss: 0.064514, Valid Acc: 0.981606, Time 00:00:33\n",
      "Epoch 100. Train Loss: 0.068510, Train Acc: 0.981576, Valid Loss: 0.066070, Valid Acc: 0.981112, Time 00:00:33\n",
      "Epoch 101. Train Loss: 0.068354, Train Acc: 0.981676, Valid Loss: 0.063647, Valid Acc: 0.981606, Time 00:00:33\n",
      "Epoch 102. Train Loss: 0.068041, Train Acc: 0.982176, Valid Loss: 0.062981, Valid Acc: 0.981507, Time 00:00:32\n",
      "Epoch 103. Train Loss: 0.067796, Train Acc: 0.981876, Valid Loss: 0.063268, Valid Acc: 0.981804, Time 00:00:33\n",
      "Epoch 104. Train Loss: 0.067757, Train Acc: 0.981893, Valid Loss: 0.063263, Valid Acc: 0.982002, Time 00:00:33\n",
      "Epoch 105. Train Loss: 0.067580, Train Acc: 0.981593, Valid Loss: 0.062163, Valid Acc: 0.981804, Time 00:00:33\n",
      "Epoch 106. Train Loss: 0.067565, Train Acc: 0.981910, Valid Loss: 0.063135, Valid Acc: 0.981705, Time 00:00:34\n",
      "Epoch 107. Train Loss: 0.067459, Train Acc: 0.981810, Valid Loss: 0.062138, Valid Acc: 0.982002, Time 00:00:32\n",
      "Epoch 108. Train Loss: 0.067158, Train Acc: 0.982093, Valid Loss: 0.062267, Valid Acc: 0.981705, Time 00:00:33\n",
      "Epoch 109. Train Loss: 0.066777, Train Acc: 0.982293, Valid Loss: 0.063529, Valid Acc: 0.982298, Time 00:00:34\n",
      "Epoch 110. Train Loss: 0.066365, Train Acc: 0.982226, Valid Loss: 0.061826, Valid Acc: 0.982199, Time 00:00:33\n",
      "Epoch 111. Train Loss: 0.066730, Train Acc: 0.982143, Valid Loss: 0.067842, Valid Acc: 0.979529, Time 00:00:35\n",
      "Epoch 112. Train Loss: 0.066504, Train Acc: 0.982359, Valid Loss: 0.061674, Valid Acc: 0.982496, Time 00:00:33\n",
      "Epoch 113. Train Loss: 0.066211, Train Acc: 0.982143, Valid Loss: 0.061637, Valid Acc: 0.982100, Time 00:00:34\n",
      "Epoch 114. Train Loss: 0.066112, Train Acc: 0.982426, Valid Loss: 0.061309, Valid Acc: 0.982694, Time 00:00:33\n",
      "Epoch 115. Train Loss: 0.066185, Train Acc: 0.982259, Valid Loss: 0.061304, Valid Acc: 0.982298, Time 00:00:35\n",
      "Epoch 116. Train Loss: 0.065976, Train Acc: 0.982343, Valid Loss: 0.061034, Valid Acc: 0.981606, Time 00:00:33\n",
      "Epoch 117. Train Loss: 0.065599, Train Acc: 0.982726, Valid Loss: 0.061344, Valid Acc: 0.982199, Time 00:00:37\n",
      "Epoch 118. Train Loss: 0.065632, Train Acc: 0.982393, Valid Loss: 0.061130, Valid Acc: 0.982496, Time 00:00:37\n",
      "Epoch 119. Train Loss: 0.065364, Train Acc: 0.982609, Valid Loss: 0.061868, Valid Acc: 0.982100, Time 00:00:36\n",
      "Epoch 120. Train Loss: 0.065298, Train Acc: 0.982626, Valid Loss: 0.065247, Valid Acc: 0.981606, Time 00:00:37\n",
      "Epoch 121. Train Loss: 0.065260, Train Acc: 0.982209, Valid Loss: 0.060862, Valid Acc: 0.982298, Time 00:00:36\n",
      "Epoch 122. Train Loss: 0.065163, Train Acc: 0.982476, Valid Loss: 0.061008, Valid Acc: 0.982100, Time 00:00:35\n",
      "Epoch 123. Train Loss: 0.064881, Train Acc: 0.982793, Valid Loss: 0.060887, Valid Acc: 0.982694, Time 00:00:34\n",
      "Epoch 124. Train Loss: 0.064934, Train Acc: 0.982809, Valid Loss: 0.060872, Valid Acc: 0.982496, Time 00:00:35\n",
      "Epoch 125. Train Loss: 0.064590, Train Acc: 0.982859, Valid Loss: 0.060541, Valid Acc: 0.982298, Time 00:00:35\n",
      "Epoch 126. Train Loss: 0.064478, Train Acc: 0.982693, Valid Loss: 0.065885, Valid Acc: 0.981210, Time 00:00:36\n",
      "Epoch 127. Train Loss: 0.064232, Train Acc: 0.983109, Valid Loss: 0.064112, Valid Acc: 0.981606, Time 00:00:34\n",
      "Epoch 128. Train Loss: 0.064111, Train Acc: 0.982892, Valid Loss: 0.059892, Valid Acc: 0.982298, Time 00:00:35\n",
      "Epoch 129. Train Loss: 0.064011, Train Acc: 0.982826, Valid Loss: 0.059790, Valid Acc: 0.982496, Time 00:00:34\n",
      "Epoch 130. Train Loss: 0.064156, Train Acc: 0.982793, Valid Loss: 0.060322, Valid Acc: 0.982694, Time 00:00:34\n",
      "Epoch 131. Train Loss: 0.063568, Train Acc: 0.983459, Valid Loss: 0.060304, Valid Acc: 0.983089, Time 00:00:36\n",
      "Epoch 132. Train Loss: 0.063559, Train Acc: 0.983426, Valid Loss: 0.059729, Valid Acc: 0.982991, Time 00:00:34\n",
      "Epoch 133. Train Loss: 0.063573, Train Acc: 0.983176, Valid Loss: 0.060529, Valid Acc: 0.982595, Time 00:00:34\n",
      "Epoch 134. Train Loss: 0.063178, Train Acc: 0.983159, Valid Loss: 0.058943, Valid Acc: 0.983287, Time 00:00:33\n",
      "Epoch 135. Train Loss: 0.063006, Train Acc: 0.983342, Valid Loss: 0.065403, Valid Acc: 0.981013, Time 00:00:33\n",
      "Epoch 136. Train Loss: 0.063130, Train Acc: 0.983176, Valid Loss: 0.059396, Valid Acc: 0.982991, Time 00:00:33\n",
      "Epoch 137. Train Loss: 0.062991, Train Acc: 0.983326, Valid Loss: 0.072067, Valid Acc: 0.979529, Time 00:00:33\n",
      "Epoch 138. Train Loss: 0.063088, Train Acc: 0.983309, Valid Loss: 0.062584, Valid Acc: 0.982397, Time 00:00:33\n",
      "Epoch 139. Train Loss: 0.062857, Train Acc: 0.983376, Valid Loss: 0.059116, Valid Acc: 0.982496, Time 00:00:34\n",
      "Epoch 140. Train Loss: 0.062707, Train Acc: 0.983259, Valid Loss: 0.058518, Valid Acc: 0.983188, Time 00:00:31\n",
      "Epoch 141. Train Loss: 0.062316, Train Acc: 0.983376, Valid Loss: 0.059056, Valid Acc: 0.982892, Time 00:00:32\n",
      "Epoch 142. Train Loss: 0.062192, Train Acc: 0.983575, Valid Loss: 0.060853, Valid Acc: 0.981903, Time 00:00:33\n",
      "Epoch 143. Train Loss: 0.062204, Train Acc: 0.983525, Valid Loss: 0.058562, Valid Acc: 0.982595, Time 00:00:31\n",
      "Epoch 144. Train Loss: 0.061961, Train Acc: 0.983475, Valid Loss: 0.058139, Valid Acc: 0.983386, Time 00:00:31\n",
      "Epoch 145. Train Loss: 0.062000, Train Acc: 0.983609, Valid Loss: 0.058637, Valid Acc: 0.983188, Time 00:00:32\n",
      "Epoch 146. Train Loss: 0.061932, Train Acc: 0.983542, Valid Loss: 0.058303, Valid Acc: 0.982892, Time 00:00:34\n",
      "Epoch 147. Train Loss: 0.061687, Train Acc: 0.983825, Valid Loss: 0.059969, Valid Acc: 0.982793, Time 00:00:31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148. Train Loss: 0.061463, Train Acc: 0.983475, Valid Loss: 0.059665, Valid Acc: 0.982397, Time 00:00:31\n",
      "Epoch 149. Train Loss: 0.061455, Train Acc: 0.983542, Valid Loss: 0.057993, Valid Acc: 0.983188, Time 00:00:32\n",
      "Epoch 150. Train Loss: 0.061261, Train Acc: 0.983475, Valid Loss: 0.058294, Valid Acc: 0.983485, Time 00:00:32\n",
      "Epoch 151. Train Loss: 0.061159, Train Acc: 0.983642, Valid Loss: 0.059799, Valid Acc: 0.982100, Time 00:00:32\n",
      "Epoch 152. Train Loss: 0.061242, Train Acc: 0.983525, Valid Loss: 0.059028, Valid Acc: 0.982694, Time 00:00:31\n",
      "Epoch 153. Train Loss: 0.061095, Train Acc: 0.984042, Valid Loss: 0.058122, Valid Acc: 0.983683, Time 00:00:31\n",
      "Epoch 154. Train Loss: 0.060888, Train Acc: 0.983759, Valid Loss: 0.057925, Valid Acc: 0.982991, Time 00:00:31\n",
      "Epoch 155. Train Loss: 0.060831, Train Acc: 0.983659, Valid Loss: 0.057964, Valid Acc: 0.982199, Time 00:00:31\n",
      "Epoch 156. Train Loss: 0.060560, Train Acc: 0.983742, Valid Loss: 0.058923, Valid Acc: 0.982892, Time 00:00:31\n",
      "Epoch 157. Train Loss: 0.060579, Train Acc: 0.983859, Valid Loss: 0.058488, Valid Acc: 0.983089, Time 00:00:32\n",
      "Epoch 158. Train Loss: 0.060562, Train Acc: 0.983792, Valid Loss: 0.058056, Valid Acc: 0.983584, Time 00:00:32\n",
      "Epoch 159. Train Loss: 0.060437, Train Acc: 0.983875, Valid Loss: 0.057353, Valid Acc: 0.983683, Time 00:00:32\n",
      "Epoch 160. Train Loss: 0.060289, Train Acc: 0.983742, Valid Loss: 0.057358, Valid Acc: 0.983188, Time 00:00:31\n",
      "Epoch 161. Train Loss: 0.060158, Train Acc: 0.984059, Valid Loss: 0.057070, Valid Acc: 0.983089, Time 00:00:31\n",
      "Epoch 162. Train Loss: 0.059963, Train Acc: 0.984108, Valid Loss: 0.057002, Valid Acc: 0.983485, Time 00:00:32\n",
      "Epoch 163. Train Loss: 0.059975, Train Acc: 0.984142, Valid Loss: 0.056987, Valid Acc: 0.983881, Time 00:00:31\n",
      "Epoch 164. Train Loss: 0.059645, Train Acc: 0.984092, Valid Loss: 0.056730, Valid Acc: 0.983683, Time 00:00:31\n",
      "Epoch 165. Train Loss: 0.059497, Train Acc: 0.984192, Valid Loss: 0.057410, Valid Acc: 0.983584, Time 00:00:31\n",
      "Epoch 166. Train Loss: 0.059311, Train Acc: 0.984125, Valid Loss: 0.058024, Valid Acc: 0.983287, Time 00:00:31\n",
      "Epoch 167. Train Loss: 0.059167, Train Acc: 0.984442, Valid Loss: 0.056210, Valid Acc: 0.983881, Time 00:00:32\n",
      "Epoch 168. Train Loss: 0.059336, Train Acc: 0.984292, Valid Loss: 0.056745, Valid Acc: 0.983979, Time 00:00:32\n",
      "Epoch 169. Train Loss: 0.059146, Train Acc: 0.984108, Valid Loss: 0.057081, Valid Acc: 0.983584, Time 00:00:32\n",
      "Epoch 170. Train Loss: 0.058978, Train Acc: 0.984442, Valid Loss: 0.057189, Valid Acc: 0.983782, Time 00:00:31\n",
      "Epoch 171. Train Loss: 0.059004, Train Acc: 0.984242, Valid Loss: 0.056461, Valid Acc: 0.983782, Time 00:00:32\n",
      "Epoch 172. Train Loss: 0.058687, Train Acc: 0.984642, Valid Loss: 0.056910, Valid Acc: 0.983485, Time 00:00:31\n",
      "Epoch 173. Train Loss: 0.058641, Train Acc: 0.984408, Valid Loss: 0.066212, Valid Acc: 0.980419, Time 00:00:32\n",
      "Epoch 174. Train Loss: 0.058659, Train Acc: 0.984492, Valid Loss: 0.056233, Valid Acc: 0.983782, Time 00:00:32\n",
      "Epoch 175. Train Loss: 0.058499, Train Acc: 0.984192, Valid Loss: 0.059146, Valid Acc: 0.982793, Time 00:00:32\n",
      "Epoch 176. Train Loss: 0.058497, Train Acc: 0.984675, Valid Loss: 0.056334, Valid Acc: 0.983683, Time 00:00:33\n",
      "Epoch 177. Train Loss: 0.058269, Train Acc: 0.984625, Valid Loss: 0.055851, Valid Acc: 0.984177, Time 00:00:32\n",
      "Epoch 178. Train Loss: 0.058092, Train Acc: 0.984208, Valid Loss: 0.056763, Valid Acc: 0.983584, Time 00:00:31\n",
      "Epoch 179. Train Loss: 0.057974, Train Acc: 0.984708, Valid Loss: 0.058149, Valid Acc: 0.982694, Time 00:00:32\n",
      "Epoch 180. Train Loss: 0.058064, Train Acc: 0.984475, Valid Loss: 0.067615, Valid Acc: 0.980914, Time 00:00:32\n",
      "Epoch 181. Train Loss: 0.057863, Train Acc: 0.984575, Valid Loss: 0.059851, Valid Acc: 0.982298, Time 00:00:32\n",
      "Epoch 182. Train Loss: 0.057681, Train Acc: 0.984725, Valid Loss: 0.054834, Valid Acc: 0.984771, Time 00:00:33\n",
      "Epoch 183. Train Loss: 0.057668, Train Acc: 0.984691, Valid Loss: 0.055830, Valid Acc: 0.983782, Time 00:00:32\n",
      "Epoch 184. Train Loss: 0.057680, Train Acc: 0.984891, Valid Loss: 0.054805, Valid Acc: 0.983881, Time 00:00:31\n",
      "Epoch 185. Train Loss: 0.057345, Train Acc: 0.984558, Valid Loss: 0.059779, Valid Acc: 0.982100, Time 00:00:31\n",
      "Epoch 186. Train Loss: 0.057313, Train Acc: 0.984925, Valid Loss: 0.055173, Valid Acc: 0.984276, Time 00:00:32\n",
      "Epoch 187. Train Loss: 0.057215, Train Acc: 0.984741, Valid Loss: 0.055457, Valid Acc: 0.984276, Time 00:00:31\n",
      "Epoch 188. Train Loss: 0.057086, Train Acc: 0.984891, Valid Loss: 0.060767, Valid Acc: 0.982397, Time 00:00:31\n",
      "Epoch 189. Train Loss: 0.056928, Train Acc: 0.984642, Valid Loss: 0.056115, Valid Acc: 0.983782, Time 00:00:31\n",
      "Epoch 190. Train Loss: 0.056846, Train Acc: 0.985191, Valid Loss: 0.057750, Valid Acc: 0.982397, Time 00:00:31\n",
      "Epoch 191. Train Loss: 0.056766, Train Acc: 0.984791, Valid Loss: 0.054995, Valid Acc: 0.983881, Time 00:00:31\n",
      "Epoch 192. Train Loss: 0.056761, Train Acc: 0.984908, Valid Loss: 0.055445, Valid Acc: 0.984177, Time 00:00:31\n",
      "Epoch 193. Train Loss: 0.056494, Train Acc: 0.984691, Valid Loss: 0.056681, Valid Acc: 0.983386, Time 00:00:31\n",
      "Epoch 194. Train Loss: 0.056387, Train Acc: 0.984825, Valid Loss: 0.056986, Valid Acc: 0.983782, Time 00:00:33\n",
      "Epoch 195. Train Loss: 0.056520, Train Acc: 0.984858, Valid Loss: 0.054431, Valid Acc: 0.984276, Time 00:00:35\n",
      "Epoch 196. Train Loss: 0.056198, Train Acc: 0.984958, Valid Loss: 0.055035, Valid Acc: 0.984276, Time 00:00:32\n",
      "Epoch 197. Train Loss: 0.056268, Train Acc: 0.985058, Valid Loss: 0.054940, Valid Acc: 0.984177, Time 00:00:31\n",
      "Epoch 198. Train Loss: 0.056018, Train Acc: 0.984858, Valid Loss: 0.055336, Valid Acc: 0.984375, Time 00:00:32\n",
      "Epoch 199. Train Loss: 0.055525, Train Acc: 0.985391, Valid Loss: 0.054518, Valid Acc: 0.984375, Time 00:00:32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train(net, train_data, test_data, 200, optimizer, loss_func,scheduler=scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
